{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import json\n",
    "from math import ceil, floor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,scale\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, Dropout, GlobalMaxPooling1D,MaxPooling1D,LSTM\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import  EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;-]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "Number_RE = re.compile('[*^0-9]')\n",
    "Bad_underline = re.compile('[*_*]')\n",
    "RemoveTag = re.compile('&lt;|br&gt;|b&gt;|ul&gt;|li&gt;')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = RemoveTag.sub('',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = Number_RE.sub(' ', text) # replace Number symbols by space in text\n",
    "    text = Bad_underline.sub(' ', text) # replace Underline symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "RemoveLastSpace = re.compile(' $')\n",
    "\n",
    "def clean_text_category(text):\n",
    "    text = RemoveLastSpace.sub('',text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNumberThreshold(arr):\n",
    "    dropCategory = []\n",
    "\n",
    "    for key,value in arr.items():\n",
    "        if(value<=30):\n",
    "            dropCategory.append(key)\n",
    "    return dropCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../walmartNewData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['longDescription'])]\n",
    "df.longDescription = df.longDescription.apply(clean_text)\n",
    "df = df[pd.notnull(df['itemId'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 117423 entries, 0 to 117597\n",
      "Data columns (total 10 columns):\n",
      "productName         117416 non-null object\n",
      "shortDescription    117269 non-null object\n",
      "longDescription     117423 non-null object\n",
      "itemId              117423 non-null int64\n",
      "category            117423 non-null int64\n",
      "subcategory         117423 non-null int64\n",
      "sub2category        117423 non-null int64\n",
      "categoryName        117423 non-null object\n",
      "subcategoryName     117423 non-null object\n",
      "sub2categoryName    117423 non-null object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.category)\n",
    "target = le.classes_\n",
    "labels = le.transform(df.category)\n",
    "\n",
    "le.fit(df.subcategory)\n",
    "subtarget = le.classes_\n",
    "sublabels = le.transform(df.subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300 # We use 100 dimensional glove vectors\n",
    "glove_dir = '../../glove.6B' # This is the folder with the dataset\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "    print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeName = [ np.unique(df[df['category']==ele]['categoryName'])[0] for ele in target]\n",
    "subtargeName =  [ np.unique(df[df['subcategory']==ele]['subcategoryName'])[0] for ele in subtarget]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_info, y_test_info = train_test_split(df.longDescription, pd.DataFrame({'index':df.index, 'label':labels}), \n",
    "                                                    test_size=0.1, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_info.label\n",
    "y_test = y_test_info.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "X = tfidfconverter.fit_transform(df.longDescription)\n",
    "vocab_size = len(tfidfconverter.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_pad_sequence (Xtrain,Xtest,vocab_size,embeddings_index):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "    tokenizer.fit_on_texts(Xtrain)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(Xtrain)\n",
    "    sequences_test = tokenizer.texts_to_sequences(Xtest)\n",
    "    \n",
    "    trainlengths = [len(ele) for ele in sequences]\n",
    "    testlengths = [len(ele) for ele in sequences_test]\n",
    "    max_length = min(max(trainlengths),max(testlengths))\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_dim = 300\n",
    "    nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    # The vectors need to be in the same position as their index. \n",
    "    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "    # Loop over all words in the word index\n",
    "    for word, i in word_index.items():\n",
    "        # If we are above the amount of words we want to use we do nothing\n",
    "        if i >= vocab_size: \n",
    "            continue\n",
    "        # Get the embedding vector for the word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # If there is an embedding vector, put it in the embedding matrix\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return sequences,sequences_test,max_length, embedding_matrix\n",
    "\n",
    "def model_settings(length,vocabSize,embeddingMatrix,outputnum):\n",
    "        embedding_dim = 300\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=length, weights = [embedding_matrix], \n",
    "                                trainable = False))\n",
    "        model.add(Conv1D(200,3,padding='valid',activation='relu',strides=1))        \n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(250))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(outputnum, activation='softmax'))\n",
    "        model.summary()\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "        return model\n",
    "def model_settings2(length,vocabSize,embeddingMatrix,outputnum):\n",
    "        embedding_dim = 300\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocabSize, embedding_dim, input_length=length, weights = [embeddingMatrix], \n",
    "                                trainable = False))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv1D(128,5,padding='valid',activation='relu',strides=1))        \n",
    "        # we use max pooling:\n",
    "        model.add(MaxPooling1D(4))\n",
    "        model.add(LSTM(70))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(outputnum, activation='softmax'))\n",
    "        model.summary()\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 14:12:59.644421 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0803 14:12:59.749700 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 14:12:59.755800 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0803 14:12:59.813936 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0803 14:12:59.822894 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0803 14:13:00.197264 4695414208 deprecation.py:506] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0803 14:13:00.295340 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0803 14:13:00.389101 4695414208 deprecation.py:323] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           8339400   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8032      \n",
      "=================================================================\n",
      "Total params: 8,577,882\n",
      "Trainable params: 238,482\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 14:13:00.880213 4695414208 deprecation_wrapper.py:119] From /Users/apple/Desktop/projects/uob-msc50-project/classification/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           8339400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 128)           192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 70)                55720     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2272      \n",
      "=================================================================\n",
      "Total params: 8,589,520\n",
      "Trainable params: 250,120\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 56s 593us/step - loss: 0.0368 - categorical_accuracy: 0.7965 - val_loss: 0.0290 - val_categorical_accuracy: 0.8435\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 51s 533us/step - loss: 0.0244 - categorical_accuracy: 0.8675 - val_loss: 0.0264 - val_categorical_accuracy: 0.8582\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 57s 597us/step - loss: 0.0199 - categorical_accuracy: 0.8930 - val_loss: 0.0275 - val_categorical_accuracy: 0.8582\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 57s 603us/step - loss: 0.0170 - categorical_accuracy: 0.9078 - val_loss: 0.0278 - val_categorical_accuracy: 0.8562\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 56s 592us/step - loss: 0.0147 - categorical_accuracy: 0.9219 - val_loss: 0.0299 - val_categorical_accuracy: 0.8583\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 57s 599us/step - loss: 0.0131 - categorical_accuracy: 0.9318 - val_loss: 0.0305 - val_categorical_accuracy: 0.8540\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 57s 602us/step - loss: 0.0120 - categorical_accuracy: 0.9388 - val_loss: 0.0321 - val_categorical_accuracy: 0.8582\n",
      "Epoch 00007: early stopping\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 88s 928us/step - loss: 0.0509 - categorical_accuracy: 0.7142 - val_loss: 0.0380 - val_categorical_accuracy: 0.7872\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 95s 1000us/step - loss: 0.0371 - categorical_accuracy: 0.7935 - val_loss: 0.0348 - val_categorical_accuracy: 0.8074\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 94s 986us/step - loss: 0.0331 - categorical_accuracy: 0.8179 - val_loss: 0.0321 - val_categorical_accuracy: 0.8252\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 93s 973us/step - loss: 0.0306 - categorical_accuracy: 0.8315 - val_loss: 0.0318 - val_categorical_accuracy: 0.8272\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 97s 1ms/step - loss: 0.0287 - categorical_accuracy: 0.8426 - val_loss: 0.0314 - val_categorical_accuracy: 0.8314\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 95s 994us/step - loss: 0.0273 - categorical_accuracy: 0.8501 - val_loss: 0.0307 - val_categorical_accuracy: 0.8321\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 94s 992us/step - loss: 0.0259 - categorical_accuracy: 0.8582 - val_loss: 0.0311 - val_categorical_accuracy: 0.8333\n",
      "Epoch 8/10\n",
      "95112/95112 [==============================] - 95s 995us/step - loss: 0.0249 - categorical_accuracy: 0.8634 - val_loss: 0.0310 - val_categorical_accuracy: 0.8371\n",
      "Epoch 9/10\n",
      "95112/95112 [==============================] - 92s 967us/step - loss: 0.0242 - categorical_accuracy: 0.8683 - val_loss: 0.0307 - val_categorical_accuracy: 0.8371\n",
      "Epoch 10/10\n",
      "95112/95112 [==============================] - 89s 932us/step - loss: 0.0235 - categorical_accuracy: 0.8722 - val_loss: 0.0306 - val_categorical_accuracy: 0.8379\n",
      "11743/11743 [==============================] - 3s 225us/step\n",
      "11743/11743 [==============================] - 4s 317us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 150, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 148, 200)          180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8032      \n",
      "=================================================================\n",
      "Total params: 8,577,882\n",
      "Trainable params: 238,482\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 146, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 70)                55720     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2272      \n",
      "=================================================================\n",
      "Total params: 8,589,520\n",
      "Trainable params: 250,120\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 165s 2ms/step - loss: 0.0351 - categorical_accuracy: 0.8071 - val_loss: 0.0268 - val_categorical_accuracy: 0.8589\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 155s 2ms/step - loss: 0.0227 - categorical_accuracy: 0.8769 - val_loss: 0.0253 - val_categorical_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 153s 2ms/step - loss: 0.0184 - categorical_accuracy: 0.9006 - val_loss: 0.0249 - val_categorical_accuracy: 0.8725\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 124s 1ms/step - loss: 0.0155 - categorical_accuracy: 0.9177 - val_loss: 0.0277 - val_categorical_accuracy: 0.8638\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 86s 906us/step - loss: 0.0136 - categorical_accuracy: 0.9279 - val_loss: 0.0272 - val_categorical_accuracy: 0.8677\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 87s 910us/step - loss: 0.0120 - categorical_accuracy: 0.9387 - val_loss: 0.0294 - val_categorical_accuracy: 0.8712\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 87s 911us/step - loss: 0.0109 - categorical_accuracy: 0.9444 - val_loss: 0.0297 - val_categorical_accuracy: 0.8687\n",
      "Epoch 8/10\n",
      "95112/95112 [==============================] - 86s 902us/step - loss: 0.0100 - categorical_accuracy: 0.9503 - val_loss: 0.0326 - val_categorical_accuracy: 0.8667\n",
      "Epoch 00008: early stopping\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 153s 2ms/step - loss: 0.0510 - categorical_accuracy: 0.7125 - val_loss: 0.0380 - val_categorical_accuracy: 0.7899\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 152s 2ms/step - loss: 0.0364 - categorical_accuracy: 0.7993 - val_loss: 0.0332 - val_categorical_accuracy: 0.8169\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 152s 2ms/step - loss: 0.0321 - categorical_accuracy: 0.8241 - val_loss: 0.0314 - val_categorical_accuracy: 0.8283\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 150s 2ms/step - loss: 0.0296 - categorical_accuracy: 0.8383 - val_loss: 0.0304 - val_categorical_accuracy: 0.8362\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 150s 2ms/step - loss: 0.0276 - categorical_accuracy: 0.8501 - val_loss: 0.0294 - val_categorical_accuracy: 0.8411\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 168s 2ms/step - loss: 0.0261 - categorical_accuracy: 0.8578 - val_loss: 0.0294 - val_categorical_accuracy: 0.8407\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 170s 2ms/step - loss: 0.0250 - categorical_accuracy: 0.8636 - val_loss: 0.0287 - val_categorical_accuracy: 0.8440\n",
      "Epoch 8/10\n",
      "95112/95112 [==============================] - 153s 2ms/step - loss: 0.0239 - categorical_accuracy: 0.8698 - val_loss: 0.0287 - val_categorical_accuracy: 0.8450\n",
      "Epoch 9/10\n",
      "95112/95112 [==============================] - 152s 2ms/step - loss: 0.0231 - categorical_accuracy: 0.8747 - val_loss: 0.0289 - val_categorical_accuracy: 0.8477\n",
      "Epoch 10/10\n",
      "95112/95112 [==============================] - 152s 2ms/step - loss: 0.0222 - categorical_accuracy: 0.8792 - val_loss: 0.0285 - val_categorical_accuracy: 0.8511\n",
      "11743/11743 [==============================] - 4s 317us/step\n",
      "11743/11743 [==============================] - 6s 540us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 248, 200)          180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                8032      \n",
      "=================================================================\n",
      "Total params: 8,577,882\n",
      "Trainable params: 238,482\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 246, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 70)                55720     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2272      \n",
      "=================================================================\n",
      "Total params: 8,589,520\n",
      "Trainable params: 250,120\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 131s 1ms/step - loss: 0.0350 - categorical_accuracy: 0.8075 - val_loss: 0.0274 - val_categorical_accuracy: 0.8520\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 132s 1ms/step - loss: 0.0224 - categorical_accuracy: 0.8780 - val_loss: 0.0252 - val_categorical_accuracy: 0.8647\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 131s 1ms/step - loss: 0.0184 - categorical_accuracy: 0.9016 - val_loss: 0.0254 - val_categorical_accuracy: 0.8689\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 132s 1ms/step - loss: 0.0155 - categorical_accuracy: 0.9180 - val_loss: 0.0288 - val_categorical_accuracy: 0.8633\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 131s 1ms/step - loss: 0.0133 - categorical_accuracy: 0.9293 - val_loss: 0.0292 - val_categorical_accuracy: 0.8653\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 130s 1ms/step - loss: 0.0120 - categorical_accuracy: 0.9383 - val_loss: 0.0297 - val_categorical_accuracy: 0.8680\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 128s 1ms/step - loss: 0.0107 - categorical_accuracy: 0.9461 - val_loss: 0.0332 - val_categorical_accuracy: 0.8519\n",
      "Epoch 00007: early stopping\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 242s 3ms/step - loss: 0.0508 - categorical_accuracy: 0.7142 - val_loss: 0.0385 - val_categorical_accuracy: 0.7872\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 240s 3ms/step - loss: 0.0364 - categorical_accuracy: 0.7972 - val_loss: 0.0335 - val_categorical_accuracy: 0.8142\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 240s 3ms/step - loss: 0.0321 - categorical_accuracy: 0.8229 - val_loss: 0.0316 - val_categorical_accuracy: 0.8303\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 241s 3ms/step - loss: 0.0293 - categorical_accuracy: 0.8395 - val_loss: 0.0302 - val_categorical_accuracy: 0.8356\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 241s 3ms/step - loss: 0.0274 - categorical_accuracy: 0.8510 - val_loss: 0.0300 - val_categorical_accuracy: 0.8396\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 241s 3ms/step - loss: 0.0259 - categorical_accuracy: 0.8587 - val_loss: 0.0292 - val_categorical_accuracy: 0.8416\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 242s 3ms/step - loss: 0.0245 - categorical_accuracy: 0.8664 - val_loss: 0.0289 - val_categorical_accuracy: 0.8444\n",
      "Epoch 8/10\n",
      "95112/95112 [==============================] - 242s 3ms/step - loss: 0.0237 - categorical_accuracy: 0.8714 - val_loss: 0.0293 - val_categorical_accuracy: 0.8453\n",
      "Epoch 9/10\n",
      "95112/95112 [==============================] - 242s 3ms/step - loss: 0.0229 - categorical_accuracy: 0.8752 - val_loss: 0.0294 - val_categorical_accuracy: 0.8443\n",
      "Epoch 10/10\n",
      "95112/95112 [==============================] - 242s 3ms/step - loss: 0.0222 - categorical_accuracy: 0.8785 - val_loss: 0.0293 - val_categorical_accuracy: 0.8469\n",
      "11743/11743 [==============================] - 6s 543us/step\n",
      "11743/11743 [==============================] - 10s 875us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 350, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 348, 200)          180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                8032      \n",
      "=================================================================\n",
      "Total params: 8,577,882\n",
      "Trainable params: 238,482\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 350, 300)          8339400   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 350, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 346, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 86, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 70)                55720     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2272      \n",
      "=================================================================\n",
      "Total params: 8,589,520\n",
      "Trainable params: 250,120\n",
      "Non-trainable params: 8,339,400\n",
      "_________________________________________________________________\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "95112/95112 [==============================] - 181s 2ms/step - loss: 0.0349 - categorical_accuracy: 0.8086 - val_loss: 0.0268 - val_categorical_accuracy: 0.8577\n",
      "Epoch 2/10\n",
      "95112/95112 [==============================] - 181s 2ms/step - loss: 0.0225 - categorical_accuracy: 0.8780 - val_loss: 0.0260 - val_categorical_accuracy: 0.8618\n",
      "Epoch 3/10\n",
      "95112/95112 [==============================] - 181s 2ms/step - loss: 0.0181 - categorical_accuracy: 0.9018 - val_loss: 0.0251 - val_categorical_accuracy: 0.8702\n",
      "Epoch 4/10\n",
      "95112/95112 [==============================] - 180s 2ms/step - loss: 0.0155 - categorical_accuracy: 0.9172 - val_loss: 0.0273 - val_categorical_accuracy: 0.8538\n",
      "Epoch 5/10\n",
      "95112/95112 [==============================] - 181s 2ms/step - loss: 0.0135 - categorical_accuracy: 0.9291 - val_loss: 0.0276 - val_categorical_accuracy: 0.8725\n",
      "Epoch 6/10\n",
      "95112/95112 [==============================] - 182s 2ms/step - loss: 0.0118 - categorical_accuracy: 0.9385 - val_loss: 0.0310 - val_categorical_accuracy: 0.8632\n",
      "Epoch 7/10\n",
      "95112/95112 [==============================] - 183s 2ms/step - loss: 0.0106 - categorical_accuracy: 0.9462 - val_loss: 0.0294 - val_categorical_accuracy: 0.8687\n",
      "Epoch 8/10\n",
      "95112/95112 [==============================] - 164s 2ms/step - loss: 0.0098 - categorical_accuracy: 0.9511 - val_loss: 0.0315 - val_categorical_accuracy: 0.8717\n",
      "Epoch 00008: early stopping\n",
      "Train on 95112 samples, validate on 10568 samples\n",
      "Epoch 1/10\n",
      "53100/95112 [===============>..............] - ETA: 2:15 - loss: 0.0577 - categorical_accuracy: 0.6760"
     ]
    }
   ],
   "source": [
    "categoricalLabel = to_categorical(y_train)\n",
    "categoricalTestLabel = to_categorical(y_test)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "\n",
    "\n",
    "sequences,sequences_test,max_length, embedding_matrix =  tokenizer_and_pad_sequence(X_train,X_test, vocab_size, embeddings_index)\n",
    "\n",
    "Scores = []\n",
    "Models = []\n",
    "Scores2 = []\n",
    "Models2 = []\n",
    "Scores3 = []\n",
    "Models3 = []\n",
    "Lengths = [ele for ele in range(50, max_length, 100)]\n",
    "for ele in range(50, max_length, 100):\n",
    "    \n",
    "    train = pad_sequences(sequences,maxlen= ele)\n",
    "    test = pad_sequences(sequences_test,maxlen = ele)\n",
    "    \n",
    "    model = model_settings(ele, vocab_size, embedding_matrix, len(np.unique(y_train)))\n",
    "    model2 = model_settings2(ele, vocab_size, embedding_matrix, len(np.unique(y_train)))\n",
    "    model.fit(train, categoricalLabel, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping],  \n",
    "                   verbose=1, validation_split=0.1)\n",
    "    model2.fit(train, categoricalLabel, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping],  \n",
    "                   verbose=1, validation_split=0.1)\n",
    "    \n",
    "    loss, score = model.evaluate(test, categoricalTestLabel, batch_size=BATCH_SIZE)\n",
    "    loss2, score2 = model2.evaluate(test, categoricalTestLabel, batch_size=BATCH_SIZE)\n",
    "\n",
    "    Scores.append(score)\n",
    "    Models.append(model)\n",
    "    Scores2.append(score2)\n",
    "    Models2.append(model2)    \n",
    "    \n",
    "bestModelIndex = np.argmax(Scores)\n",
    "bestLength = Lengths[bestModelIndex]\n",
    "bestModel = Models[bestModelIndex]\n",
    "\n",
    "bestModelIndex2 = np.argmax(Scores2)\n",
    "bestLength2 = Lengths[bestModelIndex2]\n",
    "bestModel2 = Models2[bestModelIndex2]\n",
    "\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'Walmart/'+str(bestLength)+'BestCNNs.sav'\n",
    "joblib.dump(bestModel, filename)\n",
    "filename2 = 'Walmart/'+str(bestLength2)+'BestLayerCNNswithLSTM.sav'\n",
    "joblib.dump(bestMode2, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pad_sequences(sequences_test,maxlen = bestLength)\n",
    "yConfidence = bestModel.predict(test)\n",
    "yPred = [ np.argmax(ele) for ele in yConfidence ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax  = plt.subplots()\n",
    "ax.plot(Lengths,Scores,label=\"CNNs\")\n",
    "ax.plot(Lengths,Scores2,label=\"CNNs+LSTM\")\n",
    "\n",
    "ax.set_xlabel(\"Length of pad sequence\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "fig.savefig(\"Walmart/CNNsbestsequenceLength.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalConfidenceSize = yConfidence.shape[0]*yConfidence.shape[1]\n",
    "Confidence1DArr = yConfidence.reshape(totalConfidenceSize,1)\n",
    "Max = ceil(max(Confidence1DArr)[0])\n",
    "Min = floor(min(Confidence1DArr)[0])\n",
    "NormalisedConfid = [round((max(ele)-Min)/(Max-Min),2) for ele in yConfidence]\n",
    "productID = [ df[df.index==ele]['itemId'].values[0] for ele in y_test_info['index'] ]\n",
    "ProductDescription =[ df[df.index==ele]['longDescription'].values[0] for ele in y_test_info['index'] ]\n",
    "CNNResult = pd.DataFrame({'id':productID,'description':ProductDescription,\n",
    "                         'trueClass': y_test,'trueClassNaem':[targeName[e] for e in y_test] ,\n",
    "                         'predictClass':yPred,'predictClassName':[targeName[e] for e in yPred],\n",
    "                         'Confidence':NormalisedConfid})\n",
    "CNNResult.to_csv(\"Amazon/CNNResults.csv\",index=False,compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confidenceReport(data):\n",
    "    num = [ ele*0.01 for ele in range(20,81,2)]\n",
    "    totalitems = []\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "    for ele in num:\n",
    "        Threshold = data[data['Confidence']>ele]\n",
    "        correct.append(len(Threshold[Threshold['trueClass']==Threshold['predictClass']].index))\n",
    "        incorrect.append(len(Threshold[Threshold['trueClass']!=Threshold['predictClass']].index))\n",
    "        totalitems.append(len(Threshold.index))\n",
    "    \n",
    "    ClassfiedProportion = [ ele/len(data.index) for ele in totalitems]\n",
    "    \n",
    "    Acc =[]\n",
    "    for ele in zip(correct,totalitems):\n",
    "        if (ele[1]==0):\n",
    "            Acc.append(0)\n",
    "        else:\n",
    "            Acc.append(ele[0]/ele[1])\n",
    "\n",
    "    fig,ax  = plt.subplots()\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax.scatter(num,ClassfiedProportion,label=\"Proportion classified\",color=color)\n",
    "    ax.set_xlabel(\"Confidence Score\")\n",
    "    ax.set_ylabel(\"Proportion classified\", color=color)\n",
    "    ax.xaxis.set_ticks(np.arange(0,1.1,0.1))\n",
    "    ax.yaxis.set_ticks(np.arange(0,1.1,0.1))\n",
    "    ax.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    color = 'tab:red'\n",
    "\n",
    "    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    ax2.scatter(num,Acc,label=\"Accuracy\", color=color)\n",
    "    ax2.set_ylabel(\"Accuracy\", color=color)\n",
    "    ax2.yaxis.set_ticks(np.arange(0,1.1,0.1))\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax.grid()\n",
    "    fig.savefig(\"Walmart/ConfidenceScoreReportCNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, yPred,target_names=targetName))\n",
    "with open('Walmart/CNNReport.txt', 'w') as file:\n",
    "    file.write(classification_report(y_test, yPred,target_names=targetName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confidenceReport(CNNResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im =ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "classes = targetName\n",
    "# We want to show all ticks...\n",
    "ax.set(xticks=np.arange(cm.shape[1]),yticks=np.arange(cm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "        xticklabels=classes, yticklabels=classes,ylabel='True label',xlabel='Predicted label')\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "fmt = 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"Walmart/CNNConfusionMatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
