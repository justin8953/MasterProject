{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import json\n",
    "from math import ceil, floor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,scale\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, Dropout,BatchNormalization, GlobalMaxPooling1D,MaxPooling1D,LSTM\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import  EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;-]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "Number_RE = re.compile('[*^0-9]')\n",
    "Bad_underline = re.compile('[*_*]')\n",
    "RemoveTag = re.compile('&lt;|br&gt;|b&gt;|ul&gt;|li&gt;')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = RemoveTag.sub('',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = Number_RE.sub(' ', text) # replace Number symbols by space in text\n",
    "    text = Bad_underline.sub(' ', text) # replace Underline symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "RemoveLastSpace = re.compile(' $')\n",
    "\n",
    "def clean_text_category(text):\n",
    "    text = RemoveLastSpace.sub('',text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNumberThreshold(arr):\n",
    "    dropCategory = []\n",
    "\n",
    "    for key,value in arr.items():\n",
    "        if(value<=30):\n",
    "            dropCategory.append(key)\n",
    "    return dropCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../example/amazon_co-ecommerce_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['amazon_category_and_sub_category'])]\n",
    "df = df[pd.notnull(df['description'])]\n",
    "df = df[pd.notnull(df['uniq_id'])]\n",
    "category = []\n",
    "subcategory = []\n",
    "sub2category = []\n",
    "for ele in df['amazon_category_and_sub_category'].apply(lambda x: str(x).split('>')):\n",
    "    category.append(ele[0])\n",
    "    if(len(ele)>2):\n",
    "        subcategory.append(ele[1])\n",
    "        sub2category.append(ele[2])\n",
    "    elif(len(ele)>1):\n",
    "        subcategory.append(ele[1])\n",
    "        sub2category.append(np.NaN)\n",
    "    else:\n",
    "        subcategory.append(np.NaN)\n",
    "        sub2category.append(np.NaN)\n",
    "\n",
    "data= {'uniq_id':df['uniq_id'], 'product_name':df['product_name'],'category_main':category,'category_sub1':subcategory,'description':df['description']}\n",
    "df = pd.DataFrame(data)\n",
    "df['description'] = df['description'].apply(clean_text)\n",
    "df['product_name'] = df['product_name'].apply(clean_text)\n",
    "\n",
    "\n",
    "df.category_main = df['category_main'].apply(clean_text_category)\n",
    "df.category_sub1 = df['category_sub1'].apply(clean_text_category)\n",
    "\n",
    "df = df[pd.notnull(df['category_main'])]\n",
    "df = df[pd.notnull(df['category_sub1'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCategoryCode  = classNumberThreshold(df.category_main.value_counts())\n",
    "dropSubCategoryCode  = classNumberThreshold(df.category_sub1.value_counts())\n",
    "\n",
    "for i in dropCategoryCode:\n",
    "    df = df[df.category_main!=i]\n",
    "    \n",
    "for i in dropSubCategoryCode:\n",
    "    df = df[df.category_sub1!=i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8189 entries, 0 to 9998\n",
      "Data columns (total 5 columns):\n",
      "uniq_id          8189 non-null object\n",
      "product_name     8189 non-null object\n",
      "category_main    8189 non-null object\n",
      "category_sub1    8189 non-null object\n",
      "description      8189 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 383.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300 # We use 100 dimensional glove vectors\n",
    "glove_dir = '../../glove.6B' # This is the folder with the dataset\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "    print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.category_main)\n",
    "target = le.classes_\n",
    "labels = le.transform(df.category_main)\n",
    "\n",
    "le.fit(df.category_sub1.apply(str))\n",
    "subtarget = le.classes_\n",
    "sublabels = le.transform(df['category_sub1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_info, y_test_info = train_test_split(df.product_name, pd.DataFrame({'index':df.index, 'label':labels}), \n",
    "                                                    test_size=0.1, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_info.label\n",
    "y_test = y_test_info.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "X = tfidfconverter.fit_transform(df.product_name)\n",
    "vocab_size = len(tfidfconverter.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_pad_sequence (Xtrain,Xtest,vocab_size,embeddings_index):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "    tokenizer.fit_on_texts(Xtrain)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(Xtrain)\n",
    "    sequences_test = tokenizer.texts_to_sequences(Xtest)\n",
    "    \n",
    "    trainlengths = [len(ele) for ele in sequences]\n",
    "    testlengths = [len(ele) for ele in sequences_test]\n",
    "    max_length = min(max(trainlengths),max(testlengths))\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_dim = 300\n",
    "    nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    # The vectors need to be in the same position as their index. \n",
    "    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "    # Loop over all words in the word index\n",
    "    for word, i in word_index.items():\n",
    "        # If we are above the amount of words we want to use we do nothing\n",
    "        if i >= vocab_size: \n",
    "            continue\n",
    "        # Get the embedding vector for the word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # If there is an embedding vector, put it in the embedding matrix\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return sequences,sequences_test,max_length, embedding_matrix\n",
    "\n",
    "def model_settings(length,vocabSize,embeddingMatrix,outputnum):\n",
    "        embedding_dim = 300\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=length, weights = [embedding_matrix], \n",
    "                                trainable = False))\n",
    "        model.add(Conv1D(200,3,padding='valid',activation='relu',strides=1))        \n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(BatchNormalization())\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(250))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(outputnum, activation='softmax'))\n",
    "        model.summary()\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "        return model\n",
    "def model_settings2(length,vocabSize,embeddingMatrix,outputnum):\n",
    "        embedding_dim = 300\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocabSize, embedding_dim, input_length=length, weights = [embeddingMatrix], \n",
    "                                trainable = False))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv1D(128,5,padding='valid',activation='relu',strides=1))        \n",
    "        # we use max pooling:\n",
    "        model.add(MaxPooling1D(4))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LSTM(70))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(outputnum, activation='softmax'))\n",
    "        model.summary()\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 5, 300)            597600    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 3, 200)            180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 2s 267us/step - loss: 0.1516 - categorical_accuracy: 0.5859 - val_loss: 0.1109 - val_categorical_accuracy: 0.6920\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 77us/step - loss: 0.0772 - categorical_accuracy: 0.8014 - val_loss: 0.1067 - val_categorical_accuracy: 0.7069\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 0s 73us/step - loss: 0.0501 - categorical_accuracy: 0.8726 - val_loss: 0.1087 - val_categorical_accuracy: 0.7151\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 0s 71us/step - loss: 0.0324 - categorical_accuracy: 0.9233 - val_loss: 0.1218 - val_categorical_accuracy: 0.7218\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 0s 74us/step - loss: 0.0232 - categorical_accuracy: 0.9462 - val_loss: 0.1255 - val_categorical_accuracy: 0.7205\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 76us/step - loss: 0.0191 - categorical_accuracy: 0.9581 - val_loss: 0.1335 - val_categorical_accuracy: 0.7164\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 0s 70us/step - loss: 0.0176 - categorical_accuracy: 0.9634 - val_loss: 0.1383 - val_categorical_accuracy: 0.7286\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 23us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 10, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 8, 200)            180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 2s 308us/step - loss: 0.1259 - categorical_accuracy: 0.6649 - val_loss: 0.0832 - val_categorical_accuracy: 0.7883\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 97us/step - loss: 0.0573 - categorical_accuracy: 0.8542 - val_loss: 0.0755 - val_categorical_accuracy: 0.8033\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 1s 98us/step - loss: 0.0363 - categorical_accuracy: 0.9139 - val_loss: 0.0861 - val_categorical_accuracy: 0.7978\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 1s 103us/step - loss: 0.0260 - categorical_accuracy: 0.9382 - val_loss: 0.0862 - val_categorical_accuracy: 0.8019\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 1s 98us/step - loss: 0.0181 - categorical_accuracy: 0.9578 - val_loss: 0.0865 - val_categorical_accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 97us/step - loss: 0.0143 - categorical_accuracy: 0.9662 - val_loss: 0.0935 - val_categorical_accuracy: 0.7978\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 1s 98us/step - loss: 0.0131 - categorical_accuracy: 0.9738 - val_loss: 0.1096 - val_categorical_accuracy: 0.7910\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 30us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 15, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 13, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 2s 344us/step - loss: 0.1232 - categorical_accuracy: 0.6704 - val_loss: 0.0781 - val_categorical_accuracy: 0.7938\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 131us/step - loss: 0.0533 - categorical_accuracy: 0.8652 - val_loss: 0.0812 - val_categorical_accuracy: 0.7938\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 1s 135us/step - loss: 0.0335 - categorical_accuracy: 0.9156 - val_loss: 0.0820 - val_categorical_accuracy: 0.7992\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 1s 128us/step - loss: 0.0238 - categorical_accuracy: 0.9450 - val_loss: 0.0837 - val_categorical_accuracy: 0.8155\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 1s 126us/step - loss: 0.0195 - categorical_accuracy: 0.9531 - val_loss: 0.0850 - val_categorical_accuracy: 0.8155\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 136us/step - loss: 0.0155 - categorical_accuracy: 0.9646 - val_loss: 0.1015 - val_categorical_accuracy: 0.7965\n",
      "Epoch 00006: early stopping\n",
      "819/819 [==============================] - 0s 38us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 20, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 18, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 2s 374us/step - loss: 0.1248 - categorical_accuracy: 0.6703 - val_loss: 0.0789 - val_categorical_accuracy: 0.7965\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 148us/step - loss: 0.0544 - categorical_accuracy: 0.8598 - val_loss: 0.0816 - val_categorical_accuracy: 0.7924\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 1s 149us/step - loss: 0.0343 - categorical_accuracy: 0.9142 - val_loss: 0.0826 - val_categorical_accuracy: 0.7843\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 1s 149us/step - loss: 0.0235 - categorical_accuracy: 0.9438 - val_loss: 0.0917 - val_categorical_accuracy: 0.7978\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 1s 146us/step - loss: 0.0182 - categorical_accuracy: 0.9582 - val_loss: 0.0902 - val_categorical_accuracy: 0.8100\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 142us/step - loss: 0.0149 - categorical_accuracy: 0.9664 - val_loss: 0.1008 - val_categorical_accuracy: 0.7965\n",
      "Epoch 00006: early stopping\n",
      "819/819 [==============================] - 0s 47us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 25, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 23, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 3s 396us/step - loss: 0.1255 - categorical_accuracy: 0.6612 - val_loss: 0.0827 - val_categorical_accuracy: 0.7924\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 166us/step - loss: 0.0528 - categorical_accuracy: 0.8655 - val_loss: 0.0782 - val_categorical_accuracy: 0.8073\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 1s 170us/step - loss: 0.0347 - categorical_accuracy: 0.9124 - val_loss: 0.0820 - val_categorical_accuracy: 0.8019\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 1s 170us/step - loss: 0.0233 - categorical_accuracy: 0.9433 - val_loss: 0.0866 - val_categorical_accuracy: 0.8019\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 1s 172us/step - loss: 0.0183 - categorical_accuracy: 0.9585 - val_loss: 0.0960 - val_categorical_accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 176us/step - loss: 0.0151 - categorical_accuracy: 0.9670 - val_loss: 0.1004 - val_categorical_accuracy: 0.7951\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 1s 185us/step - loss: 0.0132 - categorical_accuracy: 0.9739 - val_loss: 0.1037 - val_categorical_accuracy: 0.8060\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 61us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 30, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 28, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 3s 451us/step - loss: 0.1214 - categorical_accuracy: 0.6742 - val_loss: 0.0823 - val_categorical_accuracy: 0.7924\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 1s 203us/step - loss: 0.0533 - categorical_accuracy: 0.8648 - val_loss: 0.0754 - val_categorical_accuracy: 0.8100\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 1s 198us/step - loss: 0.0330 - categorical_accuracy: 0.9180 - val_loss: 0.0872 - val_categorical_accuracy: 0.7938\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 1s 202us/step - loss: 0.0221 - categorical_accuracy: 0.9474 - val_loss: 0.0927 - val_categorical_accuracy: 0.7978\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 1s 201us/step - loss: 0.0183 - categorical_accuracy: 0.9560 - val_loss: 0.0930 - val_categorical_accuracy: 0.8046\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 1s 202us/step - loss: 0.0164 - categorical_accuracy: 0.9650 - val_loss: 0.1046 - val_categorical_accuracy: 0.7978\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 1s 203us/step - loss: 0.0137 - categorical_accuracy: 0.9706 - val_loss: 0.1074 - val_categorical_accuracy: 0.7992\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 70us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 35, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 33, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 3s 482us/step - loss: 0.1209 - categorical_accuracy: 0.6733 - val_loss: 0.0828 - val_categorical_accuracy: 0.7734\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 2s 242us/step - loss: 0.0530 - categorical_accuracy: 0.8660 - val_loss: 0.0792 - val_categorical_accuracy: 0.8033\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 2s 258us/step - loss: 0.0352 - categorical_accuracy: 0.9139 - val_loss: 0.0799 - val_categorical_accuracy: 0.7978\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6633/6633 [==============================] - 2s 266us/step - loss: 0.0231 - categorical_accuracy: 0.9451 - val_loss: 0.0881 - val_categorical_accuracy: 0.8033\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 2s 244us/step - loss: 0.0174 - categorical_accuracy: 0.9597 - val_loss: 0.0899 - val_categorical_accuracy: 0.8100\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 2s 269us/step - loss: 0.0153 - categorical_accuracy: 0.9662 - val_loss: 0.0979 - val_categorical_accuracy: 0.7992\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 2s 227us/step - loss: 0.0115 - categorical_accuracy: 0.9757 - val_loss: 0.0979 - val_categorical_accuracy: 0.7897\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 73us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 40, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 38, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 3s 511us/step - loss: 0.1229 - categorical_accuracy: 0.6695 - val_loss: 0.0826 - val_categorical_accuracy: 0.7951\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 2s 226us/step - loss: 0.0538 - categorical_accuracy: 0.8679 - val_loss: 0.0803 - val_categorical_accuracy: 0.7965\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 2s 233us/step - loss: 0.0349 - categorical_accuracy: 0.9162 - val_loss: 0.0813 - val_categorical_accuracy: 0.7938\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 2s 242us/step - loss: 0.0243 - categorical_accuracy: 0.9392 - val_loss: 0.0894 - val_categorical_accuracy: 0.8073\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 2s 239us/step - loss: 0.0189 - categorical_accuracy: 0.9564 - val_loss: 0.0955 - val_categorical_accuracy: 0.7924\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 2s 229us/step - loss: 0.0145 - categorical_accuracy: 0.9686 - val_loss: 0.0973 - val_categorical_accuracy: 0.8046\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 2s 230us/step - loss: 0.0111 - categorical_accuracy: 0.9750 - val_loss: 0.1059 - val_categorical_accuracy: 0.7978\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 83us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 45, 300)           597600    \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 43, 200)           180200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_14 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 14)                3514      \n",
      "=================================================================\n",
      "Total params: 832,364\n",
      "Trainable params: 234,364\n",
      "Non-trainable params: 598,000\n",
      "_________________________________________________________________\n",
      "Train on 6633 samples, validate on 737 samples\n",
      "Epoch 1/10\n",
      "6633/6633 [==============================] - 4s 529us/step - loss: 0.1252 - categorical_accuracy: 0.6635 - val_loss: 0.0801 - val_categorical_accuracy: 0.7992\n",
      "Epoch 2/10\n",
      "6633/6633 [==============================] - 2s 243us/step - loss: 0.0543 - categorical_accuracy: 0.8607 - val_loss: 0.0792 - val_categorical_accuracy: 0.7897\n",
      "Epoch 3/10\n",
      "6633/6633 [==============================] - 2s 242us/step - loss: 0.0341 - categorical_accuracy: 0.9154 - val_loss: 0.0841 - val_categorical_accuracy: 0.7924\n",
      "Epoch 4/10\n",
      "6633/6633 [==============================] - 2s 243us/step - loss: 0.0243 - categorical_accuracy: 0.9408 - val_loss: 0.0882 - val_categorical_accuracy: 0.8005\n",
      "Epoch 5/10\n",
      "6633/6633 [==============================] - 2s 275us/step - loss: 0.0182 - categorical_accuracy: 0.9582 - val_loss: 0.0939 - val_categorical_accuracy: 0.7951\n",
      "Epoch 6/10\n",
      "6633/6633 [==============================] - 2s 258us/step - loss: 0.0146 - categorical_accuracy: 0.9686 - val_loss: 0.0969 - val_categorical_accuracy: 0.8019\n",
      "Epoch 7/10\n",
      "6633/6633 [==============================] - 2s 286us/step - loss: 0.0126 - categorical_accuracy: 0.9733 - val_loss: 0.0984 - val_categorical_accuracy: 0.8046\n",
      "Epoch 00007: early stopping\n",
      "819/819 [==============================] - 0s 104us/step\n"
     ]
    }
   ],
   "source": [
    "categoricalLabel = to_categorical(y_train)\n",
    "categoricalTestLabel = to_categorical(y_test)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "\n",
    "\n",
    "sequences,sequences_test,max_length, embedding_matrix =  tokenizer_and_pad_sequence(X_train,X_test, vocab_size, embeddings_index)\n",
    "\n",
    "Scores = []\n",
    "Loss = []\n",
    "for ele in range(5,50,5):\n",
    "\n",
    "    train = pad_sequences(sequences,maxlen= ele)\n",
    "    test = pad_sequences(sequences_test,maxlen = ele)\n",
    "\n",
    "    model = model_settings(ele, vocab_size, embedding_matrix, len(np.unique(y_train)))\n",
    "    # model2 = model_settings2(20, vocab_size, embedding_matrix, len(np.unique(y_train)))\n",
    "\n",
    "\n",
    "    model.fit(train, categoricalLabel, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping],  \n",
    "                       verbose=1, validation_split=0.1)\n",
    "    # model2.fit(train, categoricalLabel, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping],  \n",
    "    #                    verbose=1, validation_split=0.1)\n",
    "\n",
    "    loss, score = model.evaluate(test, categoricalTestLabel, batch_size=BATCH_SIZE)\n",
    "    Scores.append(score)\n",
    "    Loss.append(loss)\n",
    "# loss2, score2 = model2.evaluate(test, categoricalTestLabel, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(zip(Loss,Scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Loss: 0.14 Score: 0.75 \n",
      "Model 1 Loss: 0.11 Score: 0.80 \n",
      "Model 1 Loss: 0.09 Score: 0.83 \n",
      "Model 1 Loss: 0.10 Score: 0.80 \n",
      "Model 1 Loss: 0.10 Score: 0.83 \n",
      "Model 1 Loss: 0.10 Score: 0.83 \n",
      "Model 1 Loss: 0.10 Score: 0.81 \n",
      "Model 1 Loss: 0.10 Score: 0.83 \n",
      "Model 1 Loss: 0.10 Score: 0.83 \n"
     ]
    }
   ],
   "source": [
    "Result = \"\"\n",
    "\n",
    "for ele in results:\n",
    "    result = \"Model 1 Loss: {0:.2f} Score: {1:.2f} \".format(ele[0],ele[1])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "X = tfidfconverter.fit_transform(df.product_name)\n",
    "featureNames = tfidfconverter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_info, y_test_info = train_test_split(X, pd.DataFrame({'index':df.index, 'label':labels}), \n",
    "                                                    test_size=0.1, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best 50 features \n",
      "\n",
      "--- Best 150 features \n",
      "\n",
      "--- Best 250 features \n",
      "\n",
      "--- Best 350 features \n",
      "\n",
      "--- Best 450 features \n",
      "\n",
      "--- Best 550 features \n",
      "\n",
      "--- Best 650 features \n",
      "\n",
      "--- Best 750 features \n",
      "\n",
      "--- Best 850 features \n",
      "\n",
      "--- Best 950 features \n",
      "\n",
      "--- Best 1050 features \n",
      "\n",
      "--- Best 1150 features \n",
      "\n",
      "--- Best 1250 features \n",
      "\n",
      "--- Best 1350 features \n",
      "\n",
      "--- Best 1450 features \n",
      "\n",
      "--- Best 1550 features \n",
      "\n",
      "--- Best 1650 features \n",
      "\n",
      "--- Best 1750 features \n",
      "\n",
      "--- Best 1850 features \n",
      "\n",
      "--- Best 1950 features \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BestSize  = [ele for ele in range(50, len(featureNames), 100) ]\n",
    "SVCModels = []\n",
    "SVCScores = []\n",
    "SelectModels  = []\n",
    "for ele in range(50, len(featureNames), 100):\n",
    "    print(\"--- Best \"+ str(ele) + \" features \\n\")\n",
    "    selectBest = SelectKBest(chi2, k= ele)\n",
    "    model = LinearSVC(random_state=42,class_weight=\"balanced\")\n",
    "    K_best_linearsvc = Pipeline([('SelectBest', selectBest), ('linearSVC', model)])\n",
    "    K_best_linearsvc.fit(X_train,y_train)\n",
    "    score = K_best_linearsvc.score(X_test,y_test)\n",
    "    SVCModels.append(K_best_linearsvc)\n",
    "    SVCScores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Score: 0.57 \n",
      "Model 1 Score: 0.70 \n",
      "Model 1 Score: 0.74 \n",
      "Model 1 Score: 0.77 \n",
      "Model 1 Score: 0.79 \n",
      "Model 1 Score: 0.80 \n",
      "Model 1 Score: 0.79 \n",
      "Model 1 Score: 0.80 \n",
      "Model 1 Score: 0.81 \n",
      "Model 1 Score: 0.82 \n",
      "Model 1 Score: 0.83 \n",
      "Model 1 Score: 0.83 \n",
      "Model 1 Score: 0.83 \n",
      "Model 1 Score: 0.83 \n",
      "Model 1 Score: 0.83 \n",
      "Model 1 Score: 0.84 \n",
      "Model 1 Score: 0.84 \n",
      "Model 1 Score: 0.84 \n",
      "Model 1 Score: 0.84 \n",
      "Model 1 Score: 0.84 \n"
     ]
    }
   ],
   "source": [
    "for ele in SVCScores:\n",
    "    Result = \"Model 1 Score: {0:.2f} \".format(ele)\n",
    "    print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
